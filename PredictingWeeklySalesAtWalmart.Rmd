---
title: "Predicting Weekly Sales at Walmart Stores"
author: ''
date: "August 31, 2015"
output:
  pdf_document:
    keep_tex: yes
    latex_engine: lualatex
    number_sections: yes
    toc: yes
    toc_depth: 3
  html_document:
    keep_md: yes
    number_sections: yes
    theme: cerulean
    toc: yes
---


```{r setup, echo=FALSE , include=FALSE}
# Setting up the R Markdown Report
# echo & include is set to false because I do not want this on the report
# enabling cache so we don't have to run the code everytime
knitr::opts_chunk$set(cache=TRUE, out.width='450px' , dpi=200 )  
# width of the report & to prevent Scientific notations for large numbers
options(width=80 , scipen=10000)
```

```{r librariesUsed, include=FALSE}
# Grammar of Graphics Plotting Library
library(ggplot2)
# To use 'melt'
library(reshape2)
# to enable commas in graphs
library(scales)
# to get the month number from date variable
library(lubridate)
# to calculate Kurtosis
library(e1071)
# to be able to plot in grids
library(grid)
# to be able to plot in grids
library(gridExtra)
```

```{r standardErrorFunctionDeclared, echo=FALSE}
# Functions used in this Project
# Function to calculate the Standard Error
# x: the vector of numerical values
# returns the standard error of the vector
standardError <- function( x ) {
  sd( x )/sqrt( length( x ) )
}

```

```{r lagfunction, echo=FALSE}
## function to handle lag
## Since the in-built function in R to handle LAG is not working
## x - vector that needs to be lagged
## k - is the no of lags that need to be returned as a vector 
##   - may be positive or negative
##   - it returns 0 instead of NA for the missing values
## returns a vector contained the lagged data with padded 0s for missing data
lagpad <- function(x, k) {
  if( k > 0 ) {
    # It should actually be NA in the rep function
    c(rep(0, k), x)[1 : length(x)] 
  } else {
    # It should actually be NA in the rep function
    c(x[ (abs(k)+1) : length(x)] , rep(0, abs(k) ) )  
  }
}
```

```{r weekNumberCalFunc, echo=FALSE}
# Function to calculate Week
# date - the date for which the Week Number should be calculated
# returns week Number
weekNumber <- function( date ) {
  d1 <- as.Date( paste0( year(date) , "-01-01" ) )
  as.integer((date-d1)/7)+1
}
```

\pagebreak
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break

# Executive Summary
Retail stores need to be able to predict sales forecasts for the future and study the effect of strategic offers and how they affect sales, especially during holiday season. Since the number of days in holidays are limited, it becomes more challenging to be able to accurately predict how different aspects affect sales.

Businesses are trying to leverage as much as possible to gain marketshare and improve bottomline. It is imperative to be able to know how one variable affects the other and what is the degree of change.

This report specifically addresses the question of how holiday markdowns, Consumer Price Index (CPI), Fuel Price & Temperature affect the weekly sales at Walmart.

This report will focus on a Walmart Data set that has Department-wise Weekly Sales of 45 anonymized Walmart stores. It will attempt to create a predictive model and also discuss the extent to which different factors affect the sales.

Salient features of the data set:

* Over 400 Thousand data points for training and over 100 thousand data points for testing predictions
* Missing data exist - they need to be imputed values systematically to ensure that the model is good
* Weekly sales data is highly skewed to the right - adding an extra level of complexity

\pagebreak
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\graphicspath{ {Images/} }

# Introduction


## About the Solution Environment
The authors implemented this solution in R. We have used R Markdown Report to create this document. First we explore and prepare the data set before carrying out formal statistical inferences on the data set. We wrap the report by building a model to predict Weekly sales of the departments belonging to the 45 stores in this data set.


## About the Data
The data set under consideration is taken from a recruitment competition Walmart ran on Kaggle between February-May 2014. Each store has multiple departments and the end requirement is to be able to predict the sales for individual departments of each store.

The training data set has more than 400K records. The testing data set has over 100K records.


### The Challenge
The challenge is to be able to predict how different holiday price markdowns affect the various departments in the store, to model extent of impact of these markdowns.

\includegraphics{markdowns}
<IMAGE src="Images/markdowns.png" />


## Getting the Data
The data was download from Kaggle.

URL to the Kaggle Competition Site: https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting

The files available are the following:

\includegraphics[scale=.5]{DataFilesImage}
<IMAGE src="Images/DataFilesImage.png" />

### The Data Files
Here we discuss the various CSV Files that are given by Walmart.

#### train.csv
Weekly sales data set from February 05, 2010 to November 11, 2012. It contains the following fields:

* Store: store number
* Dept: the department number
* Date: week date
* Weekly_Sales: sales for the given department in the given store
* IsHoliday: whether the week is a special holiday week


#### stores.csv
Contains size and type of 45 stores (45 records).

It contains annonymized data of the store, it's size and type.

#### test.csv
The data set with similar fields as train.csv, except without Weekly_Sales. This will be used to test the model with unseen data and can be evaluated by uploading the data set to Kaggle.

#### features.csv
This data file contains additional relevant information relating to the physical and business environment around the store. The fields are as follows:

* Store: store number
* Date: the week date
* Temperature: the average temperature in the region
* Fuel_Price: cost of fuel in the region
* MarkDown1-5: data related to the markdowns that Walmart is running. Markdown data is only available after November 2011 and is not available for all stores all the time. Any missing value is marked with an NA.
* CPI - the Consumer Price Index
* Unemployment - the unemployment rate
* IsHoliday - whether the week is a special holiday week

The four holidays fall in the following weeks in the data set:

* Super Bowl: 12-Feb-10, 11-Feb-11, 10-Feb-12, 8-Feb-13
* Labor Day: 10-Sep-10, 9-Sep-11, 7-Sep-12, 6-Sep-13
* Thanksgiving: 26-Nov-10, 25-Nov-11, 23-Nov-12, 29-Nov-13
* Christmas: 31-Dec-10, 30-Dec-11, 28-Dec-12, 27-Dec-13


\pagebreak
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break

# Data Exploration and Preparation (Stage 1)

## Summary Statististics

### The Training Dataset (train)
```{r ingestTrain, include=FALSE}
# Ingesting the data from the Data folder: Training Dataset
train <- read.csv("Data/train.csv")
```
```{r convertingDateTrainDataset, include=FALSE}
# Changing the Date from "Format" type to "Date" Type 
train$Date <- as.Date(train$Date)
# Changing Store from int to factor
train$Store <- as.factor(train$Store)
# Changing Dept from int to factor
train$Dept <- as.factor(train$Dept)
```

This is the structure of the dataset
```{r trainDatasetStr, echo=FALSE}
# Structure of Train Dataset
str(train)
```

Summary statistics for the train data set is given below:
```{r trainDatasetSummary, echo=FALSE}
# Getting the summary of the Data
summary(train)
```

There is no missing data in the data set.

As discussed in the Introduction, this report contains data of 45 stores - represented by Store. There are a total of 81 departments in all identified between 1 to 99.

The starting date for training data set is ```r min(train$Date)```. It starts on a ```r weekdays(min(train$Date) )```. The last date recorded in the data set is ```r max(train$Date)```, which is also a ```r weekdays( max( train$Date ) )```. There are ```r days<- as.numeric( max( train$Date )  - min(train$Date) ); days``` days between them - so the data consists of a total of ```r days/7 + 1``` weeks of data.

```{r include=FALSE}
# removing from memory
rm(days)
```

#### Heavily Right-Skewed Weekly Sales
It is interesting to note that for some departments the <code>Weekly_Sales</code> are **negative**. Returns and special offers cause these negative sales figures.

The **Standard Deviation** for <code>Weekly_Sales</code> is ```r round( sd(train$Weekly_Sales) , 2)```. The **mean** is ```r round( mean(train$Weekly_Sales) , 2 )``` and **median** is ```r median(train$Weekly_Sales)```. The mean and the median are very far apart, indicating that the data is skewed - in this case, extremely **right-skewed**.

Since the data is highle skewed, it would be more appropriate use log transformation to remove the skew to make make the data fit the assumptions of inferential statistics. But before we do that, we need to take care of Negative and Zero Values in the data.

#### Dealing with Negative and Zero Weekly Sales
```{r dealingWithNegative0WeeklySales, echo=FALSE}
# subsetting the values that are negative and 0
train0 <- subset( train, train$Weekly_Sales <= 0 )
```
There are ```r nrow(train0)``` rows that are zero or negative.

Since Log transformation of negative numbers yeild <code>NA</code> and log transformation of <code>0</code> is a negative infinity value(<code>-Inf</code>), we need to handle these values appropriately.

* The data set represents a paltry ```r paste0( round( nrow(train0)/nrow( train )*100 , 1) , "%" )``` of the full dataset
* The absolute sum of this <code>Weekly_Sales</code> in this filtered data set is only ```r paste0( round( abs(sum( train0$Weekly_Sales ) ) / sum(train$Weekly_Sales ) *100 , 3 ) , "%" )``` of the overall sum of <code>Weekly_Sales</code> 
* The absolute maximum value of this dataset is ```r abs( min( train0$Weekly_Sales ) )```

Owing to the reasons mentioned above, it would be good to remove these observations from the dataset before continuting to do a Log Transoformation of <code>Weekly_Sales</code>.

```{r include=FALSE, echo=FALSE}
# remove train0
rm( train0 )
```

#### Log Transformation of Weekly Sales

```{r logtransformation, echo=FALSE}
# Create subset of train data set
train <- subset( train , train$Weekly_Sales > 0 )
# Make Log Transformation for Weekly_Sales
train$Log_Weekly_Sales <- log(train$Weekly_Sales)
```

The histogram below shows the the change:
```{r weeklySalesSkew, echo=FALSE}
# plotting Weekly_Sales with binwidth of 5000. 
# Mean and median are far from each other
weeklySalesHist <- ggplot( train, aes( x = Weekly_Sales ) ) +
  geom_histogram(binwidth=5000 ) + 
  # Vertical line indicating the mean value
  geom_vline( aes( xintercept = mean( Weekly_Sales ) ), color="red" ) +
  scale_y_continuous( "Frequency of Occurance" ) +
  scale_x_continuous( "Weekly Sales" )
```
```{r logWeeklySalesHistogram, echo=FALSE}
# plotting the log( Weekly_Sales ) histogram
logWeeklySalesHist <- ggplot( train, aes( x = Log_Weekly_Sales ) ) +
  geom_histogram(binwidth=.2 ) + 
  # Vertical line indicating the mean value
  geom_vline( aes( xintercept = mean( Log_Weekly_Sales ) ), color="red" ) +
  scale_y_continuous( "Frequency of Occurance" ) +
  scale_x_continuous( "log( Weekly_Sales )" )
# arranging both the historgrams in a grid
grid.arrange( weeklySalesHist , logWeeklySalesHist, nrow = 1 )
# deleting the plots
rm( weeklySalesHist , logWeeklySalesHist )
```

The <code>log(Weekly_Sales)</code> is slightly **left-skewed**, but more normal than the distribution of <code>Weekly_Sales</code>.

Detailed summary statistics on weekly sales are given below:

```{r calculatingDetailedSummaryStatisticsOfWeekly_Sales, echo=FALSE}
# Calculating Detailed Summary Statistics for Weekly_Sales
Weekly_Sales <- c( 
  mean( train$Weekly_Sales ) ,
  standardError(  train$Weekly_Sales ) ,
  median( train$Weekly_Sales) ,
  sd( train$Weekly_Sales ) ,
  var( train$Weekly_Sales ) , 
  kurtosis( train$Weekly_Sales ) ,
  skewness( train$Weekly_Sales ) ,
  range( train$Weekly_Sales )[2]-range( train$Weekly_Sales )[1] ,
  min( train$Weekly_Sales ) ,
  max( train$Weekly_Sales ) ,
  sum( train$Weekly_Sales ),
  length( train$Weekly_Sales ) 
)
```

```{r calculatingDetailedSummaryStatisticsOfLog_Weekly_Sales, echo=FALSE}
# Calculating Detailed Summary Statistics for Weekly_Sales
Log_Weekly_Sales <- c( 
  mean( train$Log_Weekly_Sales ) ,
  standardError(  train$Log_Weekly_Sales ) ,
  median( train$Log_Weekly_Sales) ,
  sd( train$Log_Weekly_Sales ) ,
  var( train$Log_Weekly_Sales ) , 
  kurtosis( train$Log_Weekly_Sales ) ,
  skewness( train$Log_Weekly_Sales ) ,
  range( train$Log_Weekly_Sales )[2]-range( train$Log_Weekly_Sales )[1] ,
  min( train$Log_Weekly_Sales ) ,
  max( train$Log_Weekly_Sales ) ,
  sum( train$Log_Weekly_Sales ),
  length( train$Log_Weekly_Sales ) 
)
```

```{r descriptionOfDefailedSummaryStatistics, echo=FALSE}
# Row Headings of the Summary statistics
Description <- c(
  "Mean",
  "Standard Error" ,
  "Median" , 
  "Standard Deviation" ,
  "Variance" , 
  "Kurtosis" ,
  "Skewness" ,
  "Range" ,
  "Min" ,
  "Max" ,
  "Sum" ,
  "Count" 
)
```

```{r printingDetailedSummaryStatisticsTable, echo=FALSE}
# Making a dataframe with the data
Detailed_Summary_Statistics_on_Weekly_Sales <- 
  data.frame( 
    Description=Description , 
    Weekly_Sales = Weekly_Sales ,
    Log_Weekly_Sales = Log_Weekly_Sales
    )
# printing out the detailed summary statistics 
print( Detailed_Summary_Statistics_on_Weekly_Sales , row.names = F )

# Removing intermediate data
rm( 
  Weekly_Sales ,  
  Log_Weekly_Sales , 
  Description , 
  Detailed_Summary_Statistics_on_Weekly_Sales )
```

The **Kurtosis** value of <code>Log_Weekly_Sales</code> indicates that it is peaked data.

### The Stores Dataset (stores)
```{r ingestStores, include=FALSE}
# Ingesting the data from the Data folder: Stores Dataset
stores <- read.csv("Data/stores.csv")
```
The structure of Stores Data Set is as follows:
```{r storesDatasetStr, echo=FALSE}
# Changing Store from int to Factor
stores$Store <- as.factor( stores$Store)
# Structure of Stores Dataset
str(stores)
```

Summmary Statistics on the data are given below:
```{r storesSummary, echo=FALSE}
# summary Statistics of Stores dataset
summary(stores)
```

There is no missing data. The stores are grouped into types, and it appears to be mostly a function of its size. The following boxplot indicates this:

```{r storeSizeBoxPlot, echo=FALSE}
# box plot to show the summary statistics of the Type of Stores and their sizes
ggplot(data=stores, 
       aes(x=Type, y=Size, fill=Type) ) + 
  geom_boxplot(outlier.shape = 15, outlier.size = 4) +
  # to show how the individual store sizes are distributed
  geom_jitter() +
  scale_y_continuous( name="Store Size" ) + 
  scale_fill_brewer( name = "Store Type" , palette = "Dark2")
```

Here are the summary statistics for each type of store:

```{r StoreSizeSummaryStatisticsBasedOnTypeSD, echo=FALSE, warning=FALSE}
# Standard Deviation of each type of Store
StoreTypeSD <- tapply( stores$Size , stores$Type , sd )
# Median of each Type of Store
StoreTypeMedian <- tapply( stores$Size , stores$Type , median )
# Mean of each type of Store
StoreTypeMean <- tapply( stores$Size , stores$Type , mean )
# Min Size of each type of Store
StoreTypeMin <- tapply( stores$Size , stores$Type , min )
# Max Size of each type of Store
StoreTypeMax <- tapply( stores$Size , stores$Type , max )
# Making Dataframe
StoreTypeSummaryStatistics <- data.frame(
  StoreTypeSD , StoreTypeMedian , StoreTypeMean , StoreTypeMin , StoreTypeMax
)
# Calculating the range
StoreTypeSummaryStatistics$Range <- StoreTypeMax-StoreTypeMin
# Printing the Summary Statistic of the Type of Stores
StoreTypeSummaryStatistics
# Removing the elements from memory
rm( StoreTypeSD , StoreTypeMedian , StoreTypeMean , 
    StoreTypeMin , StoreTypeMax, StoreTypeSummaryStatistics )
```


### The Features Dataset (features)
```{r ingestFeatures, include=FALSE}
# Ingesting the data from the Data folder: features Dataset
features <- read.csv("Data/features.csv")
```
The structure of the features data set is given below:

```{r featuresDatasetStr, echo=FALSE}
# Changing Store from int to Factor
features$Store <- as.factor( features$Store)
# Changing the Date from "Format" type to "Date" Type 
features$Date <- as.Date(features$Date)
# Structure of features dataset
str(features)
```

This data set is relevant for both the <code>train</code> and the <code>test</code> data set. Summary statistics of this data set is given below:

```{r featuresSummary, echo=FALSE}
# Summary Statistics of Features Dataset
summary(features)
```

The <code>features</code> data set has missing variables for <code>Markdown1-5</code>, <code>CPI</code> & <code>Unemployment</code>. We will discuss how we can treat these missing values below.

#### Handling Consumer Price Index (CPI) Missing Values
To undertstand how the missing values are distributed in the <code>features</code> dataset let us first plot a heat map.

```{r calculateMeanCPIAcrossAllStores, include=FALSE}
# Calculating the mean CPI Across all the stores by Date
meanCPIAcrossStores <- tapply( features$CPI , features$Date , FUN = mean )
# Converting it into Data Frame
meanCPIAcrossStoresDF <- data.frame( meanCPIAcrossStores )
# Adding the date to the data frame to see the trend
meanCPIAcrossStoresDF$Date <- as.Date( rownames( meanCPIAcrossStoresDF ) )
```

```{r CPIMissingHeatMap, warning=FALSE, echo=FALSE}
# heatmap to undersatnd where CPI is missing
missingCPIHeatMap <- ggplot( features , aes(x = Date, y = Store)) + 
  geom_tile(aes(fill = CPI)) +
  scale_fill_gradient(low="yellow", high="red" , labels = comma , name="CPI") +
  scale_y_discrete(name="Store") +
  theme( legend.position = "none" , axis.text.y= element_blank() )
# trend of Average CPI Across All Stores
avgCPiIndexTrend <- ggplot( meanCPIAcrossStoresDF , aes(x = Date, y = meanCPIAcrossStores ) ) + 
  geom_line() +
  scale_y_continuous(name="mean(CPI)" ) +
  # Adding a linear model to view the trend
  stat_smooth( method = "lm" )
# putting the graphs into a grid
grid.arrange(missingCPIHeatMap , avgCPiIndexTrend , ncol = 1 )
```

The following features stand out:

* Data for CPI is missing a little after mid-2013
* There is high variation of CPI between different stores
* But the variation over time is minimal - as the colors on the heat map suggest
* Taking an average CPI across all stores, we notice an upward linear trend

```{r buildingCPiPredictionModel, echo=FALSE}
# creating a subset of features dataset without NA Values
features1 <- subset( features , is.na( features$CPI ) == FALSE )
# creating a subset of features dataset with NA Values
features2 <- subset( features , is.na( features$CPI ) == TRUE )
# Creating Linear regression model to predict CPI
cpiPredictor <- lm( 
  formula = 
    CPI ~ 
    Date + 
    Store + 
    IsHoliday , 
  data = features1 )
```
 
It is not feasible to fetch the missing CPI Value for anonymised store data, therefore, we built a linear model (with R^2^=```r summary(cpiPredictor)$r.squared```) to predict the CPI Value for the missing data. The following graph shows how the prediction has followed the linear trend depicted earlier.

```{r predictingCPIValues, echo=FALSE}
# Predicting values for CPI to substitute the missing values
features2$CPI <- predict( cpiPredictor,  newdata = features2 )
# combining the 2 datasets
features1 <- rbind( features1 , features2)

# Calculating the mean CPI Across all the stores by Date
meanCPIAcrossStores <- tapply( features1$CPI , features1$Date , FUN = mean )
# Converting it into Data Frame
meanCPIAcrossStoresDF <- data.frame( meanCPIAcrossStores )
# Adding the date to the data frame to see the trend
meanCPIAcrossStoresDF$Date <- as.Date( rownames( meanCPIAcrossStoresDF ) )

# heatmap to check if CPI is correctly imputed
newCPIHeatMap <- ggplot( features1 , aes(x = Date, y = Store)) + 
  geom_tile(aes(fill = CPI)) +
  scale_fill_gradient(low="yellow", high="red" , labels = comma , name="CPI") +
  scale_y_discrete(name="Store" ) +
  theme( legend.position = "none" , axis.text.y= element_blank() )
# trend of Average CPI Across All Stores
avgCPiIndexTrend <- ggplot( meanCPIAcrossStoresDF , aes(x = Date, y = meanCPIAcrossStores ) ) + 
  geom_line() +
  scale_y_continuous(name="mean(CPI)" ) +
  stat_smooth( method = "lm" )
grid.arrange(newCPIHeatMap , avgCPiIndexTrend , ncol = 1 )

# overwriting old features with features1 (with no missing values for CPI)
features <- features1

# removing variables not needed anymore
rm( missingCPIHeatMap , avgCPiIndexTrend , meanCPIAcrossStores ,  
    cpiPredictor , features1 , features2 , newCPIHeatMap )
```

#### Handling Unemployment Missing Values
```{r unemploymentPredictor, echo=FALSE}
# mean unemployment across stores
meanUnemploymentAcrossStores <- 
  tapply( features$Unemployment , features$Date , FUN = mean )
# Converting it into Data Frame
meanUnemploymentAcrossStoresDF <- 
  data.frame( meanUnemploymentAcrossStores )
# Adding the date to the data frame to see the trend
meanUnemploymentAcrossStoresDF$Date <- 
  as.Date( rownames( meanUnemploymentAcrossStoresDF ) )

# heatmap to undersatnd where Unemployment is missing
oldUnemploymentData <- ggplot( features , aes(x = Date, y = Store)) + 
  geom_tile(aes(fill = Unemployment)) +
  scale_fill_gradient(
    low="yellow", high="red" , labels = comma , name="Unemployment") +
  scale_y_discrete(name="Store") +
  theme( legend.position = "none" , axis.text.y= element_blank() )

# trend of Average Unemployment Across All Stores
avgUnemploymentIndexTrend <- 
  ggplot( meanUnemploymentAcrossStoresDF , 
          aes(x = Date, y = meanUnemploymentAcrossStores ) ) + 
  geom_line() +
  scale_y_continuous(name="mean(Unemployment)" ) +
  stat_smooth( method = "lm" )

# creating a subset of features dataset without NA Values
features1 <- subset( features , is.na( features$Unemployment ) == FALSE )
# creating a subset of features dataset with NA Values
features2 <- subset( features , is.na( features$Unemployment ) == TRUE )


# Creating Linear regression model to predict unemployment
unemploymentPredictor <- lm( 
  formula = 
    Unemployment ~ 
    Date + 
    Store + 
    IsHoliday +
    month( Date ) + 
    year( Date ) +
    weekNumber( Date ) +
    CPI , 
  data = features1 )

# Predicting values for CPI to substitute the missing values
features2$Unemployment <- predict( unemploymentPredictor,  newdata = features2 )
## combining the 2 datasets
features1 <- rbind( features1 , features2)


# mean unemployment across stores
meanUnemploymentAcrossStores <- 
  tapply( features1$Unemployment , features$Date , FUN = mean )
# Converting it into Data Frame
meanUnemploymentAcrossStoresDF <- 
  data.frame( meanUnemploymentAcrossStores )
# Adding the date to the data frame to see the trend
meanUnemploymentAcrossStoresDF$Date <- 
  as.Date( rownames( meanUnemploymentAcrossStoresDF ) )

# heatmap to check if CPI is correctly imputed
newUnemploymentHeatMap <- ggplot( features1 , aes(x = Date, y = Store)) + 
  geom_tile( aes( fill = Unemployment ) ) +
  scale_fill_gradient(low="yellow", high="red" , labels = comma , name="Unemployment") +
  scale_y_discrete(name="Store" ) +
  theme( legend.position = "none" , axis.text.y= element_blank() )
# trend of Average CPI Across All Stores
newAvgUnemploymentIndexTrend <- 
  ggplot( meanUnemploymentAcrossStoresDF , aes(x = Date, y = meanUnemploymentAcrossStores ) ) + 
  geom_line() +
  scale_y_continuous(name="mean(Unemployment)" ) +
  stat_smooth( method = "lm" )
```
Similar to CPI index, Unemployment data is missing for the same data range. We will use a similar technique to predict Unemployment. Unemployment has a negative linear trend with respect to time. The multi-linear regression predictor for <code>Unemployment</code> has an R^2^ = ```r summary(unemploymentPredictor)$r.squared```.

```{r graphingUnemploymentPrediction, echo=FALSE, warning=FALSE}
# Arranging 4 graphs in a grid
grid.arrange( 
  oldUnemploymentData , avgUnemploymentIndexTrend , 
  newUnemploymentHeatMap , newAvgUnemploymentIndexTrend , 
  ncol = 2 , nrow = 2 )
```

```{r unemploymentExit, echo=FALSE, warning=FALSE}
# replacing the new data into the variable
features <- features1
# removing variables not needed anymore
rm( oldUnemploymentData , avgUnemploymentIndexTrend , 
    meanUnemploymentAcrossStores ,  meanUnemploymentAcrossStoresDF , 
    unemploymentPredictor , features1 , features2 , newUnemploymentHeatMap, 
    newAvgUnemploymentIndexTrend )
```



### The Test Dataset (test)
```{r ingestTest, include=FALSE}
# Ingesting the data from the Data folder: testing Dataset
test <- read.csv("Data/test.csv")
```

```{r testDatasetStr}
## Structure of test dataset
str(test)
```

There are 39 dates in total.

```{r testSummary}
## Changing the Date from "Format" type to "Date" Type 
test$Date <- as.Date(test$Date)
## Summary Statistics of test Dataset
summary(test)
```

\pagebreak

## Data Preparation - Merging the Datasets
### Merging Train and Stores Datasets
Since the <code>Type</code> & <code>Size</code> variables may influence the Weekly Sales, we are merging the <code>train</code> & <stores</code> data sets. We merge the data by <code>Store</code>.

```{r mergeTrainStores}
## Merging train and stores by Store
trainStoresMerge <- merge(train , stores , by = "Store")
```

### 3.2.2 Merging Train, Stores and Features Datasets
Since <code>Markdown1-5</code> and other variables could play an important role at predicting <code>Weekly_Sales</code>, this should be merged with the <code>trainStoresMerge</code> data set. We merge the data by <code>Store</code> & <code>Date</code>.

```{r mergeTrainStoresFeatures}
## Merging trainStoresMerge and features datasets
trainStoresFeaturesMerge <- 
  merge( trainStoresMerge , features , by = c( "Store" , "Date" ) )
## Clearing memory - removing intermediate datasets
rm( trainStoresMerge )
## Fixing the name of the Column
colnames(trainStoresFeaturesMerge)[5] <- "IsHoliday"
trainStoresFeaturesMerge$IsHoliday.y <- NULL
```

### 3.2.3 Merging Test, Stores and Features Datasets
We similarly merge the <code>test</code>, <code>stores</code> & <code>features</code> to create the <code>testStoresFeaturesMerge</code> data set.

```{r mergeTestStoresFeatures}
## Merging test and stores by Store
testStoresMerge <- merge(test , stores , by = "Store")
## Merging testStoresMerge and features datasets
testStoresFeaturesMerge <- 
  merge( testStoresMerge , features , by = c( "Store" , "Date" ) )
## Clearing Memory - removing intermediate Datasets
rm( testStoresMerge )
## Fixing the name of the Column
colnames(testStoresFeaturesMerge)[5] <- "IsHoliday"
testStoresFeaturesMerge$IsHoliday.y <- NULL
```

\pagebreak

## 3.3 Data Exploration

### 3.3.1 Total Sales Per Department in each Store
The final goal of this report is to be able to predict the weekly sales for each department in a store. First we would like to understand which departments are present in the 45 different stores and their total sales.

```{r salesDeptStore}
## running the sum function for each store & department
storeDeptTotalSales <- tapply( 
  trainStoresFeaturesMerge$Weekly_Sales , 
  trainStoresFeaturesMerge[, c("Store","Dept")]  , 
  FUN = sum )
## Converting the matrix to a dataframe
storeDeptTotalSalesDataFrame <- as.data.frame( storeDeptTotalSales )
## Setting the Store Number into the table so we can analyze it further
storeDeptTotalSalesDataFrame$Store <- 
  as.integer( rownames( storeDeptTotalSalesDataFrame ) )
## Move Store to the 1st column in the dataframe
storeDeptTotalSalesDataFrame <- 
  storeDeptTotalSalesDataFrame[ , c( ncol(storeDeptTotalSalesDataFrame) , 1:ncol(storeDeptTotalSalesDataFrame)-1 )]
## Melting the columns into rows to enable analysis
storeDeptTotalSalesDataFrame <- 
  melt(storeDeptTotalSalesDataFrame , id="Store" )
## removing the NA variables - where the department does not exist in a store
storeDeptTotalSalesDataFrame <- storeDeptTotalSalesDataFrame[ complete.cases(storeDeptTotalSalesDataFrame),]
## Renaming the Columns in the Dataframe
colnames( storeDeptTotalSalesDataFrame )[2:3] <- c("Dept" , "TotalSales" )
## Changing the Dept Type from String to Numeric
storeDeptTotalSalesDataFrame$Dept <- 
  as.integer(storeDeptTotalSalesDataFrame$Dept)
## printing out summary statistics
summary( storeDeptTotalSalesDataFrame)
## Freeing Memory
rm( storeDeptTotalSales )
```

#### 3.3.1.1 Heat Map - Store & Department Total Sales

We generate a heat map to study this interaction

```{r heatmapStoreDept}
## Generating a Heatmap of the Department's Total Sales in each of 45 stores
ggplot( storeDeptTotalSalesDataFrame , aes(x = Store, y = Dept)) + 
  geom_tile(aes(fill = TotalSales)) +
  scale_fill_gradient(
    low="yellow", high="red" , labels = comma , name="Total Sales") +
  scale_y_continuous(name="Department")
```
<BR>
From the heat map we can draw the following broad conclusions:

* The departments between 70-80 account for more sales than other departments
* Some departments are missing in some stores

```{r removeStoreDeptTotalSalesDataFrame}
## removing dataframe to free up memory
rm( storeDeptTotalSalesDataFrame )
```


### 3.3.2 Store Total Sales Vs. Size
Plotting the total sales of a store vs. Store Size. We first calculate the total sales per Store and plot it as a response (y-axis) to the Store size (x-axis) to understand the relationship between them.

```{r salesStoreSize}
## Total Sales vs. Store Size - plotting the relationship
## calculating the sum of all the store sales
StoreTotalSales <- 
  tapply(
    trainStoresFeaturesMerge$Weekly_Sales, 
    trainStoresFeaturesMerge$Store, 
    FUN = sum)
## converting the table to a DataFrame
stores$TotalSales <- StoreTotalSales
stores$TotalSalesInMillion <- stores$TotalSales/1000000
rm( StoreTotalSales )
```

```{r plotStoreSalesType}
## Plotting the Total Sales vs. Store Size
scatterPlotStoreSize <- ggplot( stores , aes(x=Size , y=TotalSalesInMillion , color = Type ) ) + 
  geom_point( size=3) +  
  scale_y_continuous(name="Total Sales in Millions" ) + 
  scale_color_brewer(palette = "Dark2", name="Store Type" ) +
  theme( legend.position = "bottom" )
```

```{r storeTypeSummaryStatistics}
## box plot to show the summary statistics of the Type of Stores
boxplotStoreSize <- ggplot(data=stores, 
       aes(x=Type, y=TotalSalesInMillion, fill=Type) ) + 
  geom_boxplot(outlier.shape = 15, outlier.size = 4) +
  ## to show how the individual store sales are distributed
  geom_jitter() +
  scale_y_continuous(name="Total Sales in Millions" ) +
  scale_fill_brewer(name = "Store Type" , palette = "Dark2") +
  theme( legend.position = "bottom" )
```

```{r storeTypeScatterBoxGrid}
## arranging both the plots in one grid
grid.arrange( scatterPlotStoreSize , boxplotStoreSize , nrow = 1 )

## removing the plots from memory
rm( scatterPlotStoreSize , boxplotStoreSize )
```

This plot indicates that there is a postie relationship between the size of the store and total sales. Also Type 'A' Stores are mostly larger stores with bigger sales and Type 'C' Stores are small with lower sales.




```{r printingOutSummaryStatisticsOfTypeOfStore}
## calculating the summary statistics for each Type
tabledTypeWiseSummaryStatistics <- 
  tapply(stores$TotalSalesInMillion , stores$Type , summary)

## Changing the Labels of the tabled Summary Statistics for printing
attributes(tabledTypeWiseSummaryStatistics)$dimnames[[1]] <- 
  c( 
    "Type A Store Summary Statistics" , 
    "Type B Store Summary Statistics" , 
    "Type C Store Summary Statistics" )

## Printing Summary Statistics for each Type of Store (based on Total Sales)
tabledTypeWiseSummaryStatistics
## Removing table - not needed for further calculations
rm( tabledTypeWiseSummaryStatistics )
```

From the graph, we can notice a Type B Outlier. A value is considered an outlier when it's more than 3 Standard Deviations from the mean.

From this we can hypothesize that the <code>Type</code> of store could be an important predictor of <code>Weekly_Sales</code>.

### Total Sales Per Week - Time Series
We discuss here the effect holidays have on Total Sales of 45 Stores.

```{r totalSalesTimeSeries}
## Running tapply with sum to find the total sales per week
totalSalesPerWeek <- 
  tapply( 
    trainStoresFeaturesMerge$Weekly_Sales , 
    trainStoresFeaturesMerge$Date , 
    FUN = sum )
## Converting table to Data Frame
totalSalesPerWeekDataFrame <- as.data.frame( totalSalesPerWeek )
## Converting date from String-Factor to Date Type
totalSalesPerWeekDataFrame$Date <- 
  as.Date( rownames(totalSalesPerWeekDataFrame ) )
## Renaming the Column to "TotalSales"
colnames(totalSalesPerWeekDataFrame)[1] <- "TotalSales"
## Calculating the Total sales in Millions
totalSalesPerWeekDataFrame$TotalSalesInMillion = 
  totalSalesPerWeekDataFrame$TotalSales/1000000
```




```{r holidayList}
## Getting the holiday List
## Extracting the Holiday List
holidayDateTable <- 
  table(trainStoresFeaturesMerge$Date , trainStoresFeaturesMerge$IsHoliday)
## Converting from Table to Data Frame
holidayDateTableDataFrame <- as.data.frame( holidayDateTable)
## Extracting the Holidays
holidayDateTableDataFrame <- 
  subset( holidayDateTableDataFrame,holidayDateTableDataFrame$Var2==T)
## Marking the Holdiays in the dataset
holidayDateTableDataFrame$IsHoliday <- 
  ifelse( holidayDateTableDataFrame$Freq >0 , 1 , 0 )
## Converting Date from String-Factor to Date Type
holidayDateTableDataFrame$Date <- as.Date( holidayDateTableDataFrame$Var1 )
# creating the holiday season - before and after the holiday week - 2 weeks
holidayDateTableDataFrame$Week1BeforeHoliday <- 
  lagpad(holidayDateTableDataFrame$IsHoliday , -1)
holidayDateTableDataFrame$Week2BeforeHoliday <- 
  lagpad(holidayDateTableDataFrame$IsHoliday , -2)
holidayDateTableDataFrame$Week1AfterHoliday <- 
  lagpad(holidayDateTableDataFrame$IsHoliday , 1)
holidayDateTableDataFrame$Week2AfterHoliday <- 
  lagpad(holidayDateTableDataFrame$IsHoliday , 2)
## Creating an ordered Holiday Season Type
holidayDateTableDataFrame$HolidaySeasonType = 
  ifelse( holidayDateTableDataFrame$Week1BeforeHoliday == 1 , 
          "1 Week Before" ,
          ifelse( 
            holidayDateTableDataFrame$Week2BeforeHoliday == 1 , 
            "2 Weeks Before" ,
                  ifelse( 
                    holidayDateTableDataFrame$Week1AfterHoliday == 1 , 
                    "1 Week After" ,
                          ifelse( 
                            holidayDateTableDataFrame$Week2AfterHoliday == 1 ,
                            "2 Weeks After" ,
                                  ifelse( 
                                    holidayDateTableDataFrame$IsHoliday == 1 , 
                                          "Holiday Week" , "No Holiday" )))))
holidayDateTableDataFrame$HolidaySeasonType = factor(
  holidayDateTableDataFrame$HolidaySeasonType , 
  ordered=TRUE , 
  levels=c( 
    "No Holiday" ,
    "2 Weeks Before" ,
    "1 Week Before" ,
    "Holiday Week" ,
    "1 Week After" ,
    "2 Weeks After"
    )
  )

## Creating a variable to mark if it is a Holiday season - 
## that is 2 weeks before + holdiay week + 2 weeks after
holidayDateTableDataFrame$IsHolidaySeason <- 
  holidayDateTableDataFrame$IsHoliday +
  holidayDateTableDataFrame$Week1BeforeHoliday * .6 +
  holidayDateTableDataFrame$Week2BeforeHoliday *.2   +
  holidayDateTableDataFrame$Week1AfterHoliday * .6 +
  holidayDateTableDataFrame$Week2AfterHoliday *.2

## Removing unneccesary Columns
holidayDateTableDataFrame$Var1 = 
  holidayDateTableDataFrame$Var2 = 
  holidayDateTableDataFrame$Freq = NULL
## Clearing Memory - removing intermediate Tables 
rm( holidayDateTable , totalSalesPerWeek )
```


```{r merging salesperWeekHolidayList}
## Mering the sales per week and holiday list 
totalSalesPerWeekDataFrame <- 
  merge( totalSalesPerWeekDataFrame, holidayDateTableDataFrame , by=2)
```

```{r plottingSalesPerWeek}
# Plotting Sales Per Week
ggplot( totalSalesPerWeekDataFrame , 
        aes(x=Date , y=TotalSalesInMillion , color = HolidaySeasonType ) ) + 
  geom_line( aes(group=1) ) + 
  geom_point(size = 2) +
  scale_y_continuous(name="Total Sales in Millions" ) +
  scale_color_brewer(palette="Dark2" , name = "Season Type")
```

We can clearly see some trends here:

* Sales go up a week before the holiday week
* It is a repeating pattern - towards the end of the year, there is more sales - perhaps because of Thanksgiving and Christmas.
* Average Total Sales per week is between 45-50 million Dollars (for 45 stores in the dataset)
* Perhaps the Data given incorrectly marks the Holiday Week for Christmas - it is marked as the dates 27, 28, 30 & 31 across different years. But looking at the peak, the highest sales for Christmas is the week before - which is perhaps the more accurate holiday week
* To be able to make better predictors, perhaps it would help to actually create separate factors for each of the holidays
* It will be interesting to study the period between Aug 27, 2010 to Feb 25, 2011. This section has all the holidays - starting with Labor Day, Thanksgiving, Christmas & Super Bowl. Since the dataset will be smaller, we will perhaps understand the data better.

```{r creatingDummyVariablesForHolidays}
#####################################
# Creating separate holiday dummy variables
holidayDateTableDataFrame$IsHolidayDefined <- 
  ifelse( 
    holidayDateTableDataFrame$IsHoliday == 1 & 
      month( holidayDateTableDataFrame$Date ) == 2 ,
          2, ifelse( 
            holidayDateTableDataFrame$IsHoliday == 1 & 
              month( holidayDateTableDataFrame$Date ) == 9 ,
                     9 , ifelse( 
                       holidayDateTableDataFrame$IsHoliday == 1 & 
                         month( holidayDateTableDataFrame$Date ) == 11 ,
                                 11 , ifelse( 
                                   holidayDateTableDataFrame$IsHoliday == 1 & 
                                     month( 
                                       holidayDateTableDataFrame$Date ) == 12 ,
                                              12 , 0 ) ) ) )
# creating the holiday season lag - before and after the holiday week - 2 weeks
holidayDateTableDataFrame$Week1BeforeHoliday <- 
  lagpad(holidayDateTableDataFrame$IsHolidayDefined , -1)
holidayDateTableDataFrame$Week2BeforeHoliday <- 
  lagpad(holidayDateTableDataFrame$IsHolidayDefined , -2)
holidayDateTableDataFrame$Week1AfterHoliday <- 
  lagpad(holidayDateTableDataFrame$IsHolidayDefined , 1)
holidayDateTableDataFrame$Week2AfterHoliday <- 
  lagpad(holidayDateTableDataFrame$IsHolidayDefined , 2)

## Creating a variable to hold the holiday type season id
holidayDateTableDataFrame$HolidaySeasonId <- as.factor(
  holidayDateTableDataFrame$Week1BeforeHoliday + 
  holidayDateTableDataFrame$Week2BeforeHoliday +
  holidayDateTableDataFrame$IsHolidayDefined +
  holidayDateTableDataFrame$Week1AfterHoliday +
  holidayDateTableDataFrame$Week2AfterHoliday )


## Creating an ordered Holiday Season Type for Super Bowl
holidayDateTableDataFrame$HolidaySeasonType <- 
  ifelse( 
    holidayDateTableDataFrame$Week1BeforeHoliday == 2 , 
    "1 Week Before Super Bowl" ,
          ifelse( 
            holidayDateTableDataFrame$Week2BeforeHoliday == 2 , 
            "2 Weeks Before Super Bowl" ,
                  ifelse( 
                    holidayDateTableDataFrame$Week1AfterHoliday == 2 , 
                    "1 Week After Super Bowl" ,
                          ifelse( 
                            holidayDateTableDataFrame$Week2AfterHoliday == 2 ,
                            "2 Weeks After Super Bowl" ,
                                  ifelse( 
                                    holidayDateTableDataFrame$IsHolidayDefined 
                                    == 2 ,
                                    "Super Bowl" , "No Holiday" )))))

## Creating an ordered Holiday Season Type for Labor Day
holidayDateTableDataFrame$HolidaySeasonType <- 
  ifelse( 
    holidayDateTableDataFrame$Week1BeforeHoliday == 9 , 
    "1 Week Before Labor Day" ,
          ifelse( 
            holidayDateTableDataFrame$Week2BeforeHoliday == 9 , 
            "2 Weeks Before Labor Day" ,
                  ifelse( 
                    holidayDateTableDataFrame$Week1AfterHoliday == 9 , 
                    "1 Week After Labor Day" ,
                          ifelse( 
                            holidayDateTableDataFrame$Week2AfterHoliday == 9 ,
                            "2 Weeks After Labor Day" ,
                                  ifelse( 
                                    holidayDateTableDataFrame$IsHolidayDefined 
                                    == 9 , "Labor Day" ,
                                    holidayDateTableDataFrame$HolidaySeasonType 
                                    ) ) ) ) )

## Creating an ordered Holiday Season Type for Thanksgiving
holidayDateTableDataFrame$HolidaySeasonType <- 
  ifelse( 
    holidayDateTableDataFrame$Week1BeforeHoliday == 11 , 
    "1 Week Before Thanksgiving" ,
          ifelse( 
            holidayDateTableDataFrame$Week2BeforeHoliday == 11 , 
            "2 Weeks Before Thanksgiving" ,
                  ifelse( 
                    holidayDateTableDataFrame$Week1AfterHoliday == 11 , 
                    "1 Week After Thanksgiving" ,
                          ifelse( 
                            holidayDateTableDataFrame$Week2AfterHoliday == 11 ,
                            "2 Weeks After Thanksgiving" ,
                                  ifelse( 
                                    holidayDateTableDataFrame$IsHolidayDefined 
                                    == 11 , "Thanksgiving" ,
                                    holidayDateTableDataFrame$HolidaySeasonType 
                                    ) ) ) ) )

## Creating an ordered Holiday Season Type for Christmas
holidayDateTableDataFrame$HolidaySeasonType <- 
  ifelse( 
    holidayDateTableDataFrame$Week1BeforeHoliday == 12 , 
    "1 Week Before Christmas" ,
          ifelse( 
            holidayDateTableDataFrame$Week2BeforeHoliday == 12 , 
            "2 Weeks Before Christmas" ,
                  ifelse( 
                    holidayDateTableDataFrame$Week1AfterHoliday == 12 , 
                    "1 Week After Christmas" ,
                          ifelse( 
                            holidayDateTableDataFrame$Week2AfterHoliday == 12 , 
                            "2 Weeks After Christmas" ,
                                  ifelse( 
                                    holidayDateTableDataFrame$IsHolidayDefined 
                                    == 12 , "Christmas" ,
                                    holidayDateTableDataFrame$HolidaySeasonType 
                                    ) ) ) ) )

holidayDateTableDataFrame$HolidaySeasonType = factor(
  holidayDateTableDataFrame$HolidaySeasonType , 
  ordered=TRUE , 
  levels=c( 
    "No Holiday" ,
    "2 Weeks Before Super Bowl" ,
    "1 Week Before Super Bowl" ,
    "Super Bowl" ,
    "1 Week After Super Bowl" ,
    "2 Weeks After Super Bowl" ,
    "2 Weeks Before Labor Day" ,
    "1 Week Before Labor Day" ,
    "Labor Day" ,
    "1 Week After Labor Day" ,
    "2 Weeks After Labor Day" ,
    "2 Weeks Before Thanksgiving" ,
    "1 Week Before Thanksgiving" ,
    "Thanksgiving" ,
    "1 Week After Thanksgiving" ,
    "2 Weeks After Thanksgiving" ,
    "2 Weeks Before Christmas" ,
    "1 Week Before Christmas" ,
    "Christmas" ,
    "1 Week After Christmas" ,
    "2 Weeks After Christmas"
  )
)
```

```{r mergeAgain}
## Mering the sales per week and holiday list 
totalSalesPerWeekDataFrame$HolidaySeasonType <- 
  holidayDateTableDataFrame$HolidaySeasonType
## Merging the season Type with sales per week
totalSalesPerWeekDataFrame$HolidaySeasonId <- 
  holidayDateTableDataFrame$HolidaySeasonId
```


```{r holidayStudySubsetting}
## Subsetting only the holidays
totalSalesPerWeekDataFrameDuringHolidays <- 
  subset( totalSalesPerWeekDataFrame , 
          totalSalesPerWeekDataFrame$Date >= '2010-08-27' & 
            totalSalesPerWeekDataFrame$Date <= '2011-02-25' )
```

```{r plottingSubsetOfHolidays}
## Plotting the subset of totalSalesPerWeekDataFrame
ggplot( totalSalesPerWeekDataFrameDuringHolidays , 
        aes(x=Date , y=TotalSalesInMillion , 
            color = HolidaySeasonId ) ) + 
  geom_line( aes(group=1) ) + 
  geom_point(size = 2) +
  scale_y_continuous(name="Total Sales in Millions" )  +
  scale_color_brewer(
    palette="Dark2" , 
    name = "Holiday Season" ,
    labels = c( 
      "No Holiday" , 
      "Super Bowl" ,
      "Labor Day" , 
      "Thanksgiving" ,
      "Christmas"
      )
    ) 
```


```{r removingUnnecessaryColumns}
## removing the following columns because it may cause 
## multi-collinearity issues once merged with the main data and
## building a model with that
holidayDateTableDataFrame$Week1BeforeHoliday = NULL
holidayDateTableDataFrame$Week2BeforeHoliday = NULL
holidayDateTableDataFrame$Week1AfterHoliday = NULL
holidayDateTableDataFrame$Week2AfterHoliday = NULL
holidayDateTableDataFrame$IsHoliday = NULL
holidayDateTableDataFrame$IsHolidayDefined = NULL
## freeing memory
rm( totalSalesPerWeekDataFrame , totalSalesPerWeekDataFrameDuringHolidays )
```

### Store-Department-wise Sales per Week - Time Series
To see a representation of the granularity of the data, we would like to plot all the data points of Weekly Sales vs Time (Week)

```{r allPoint, echo=FALSE}
# Decided not to use this while generating the PDF since it made the report
# extremely bulky with a PDF image of 18.2 Mb. Instead generated the image
# as a PNG via HTML export and embedding the image into the report
# 
# plotting all the Weekly Sales figures - colored by Dept
#ggplot(trainStoresFeaturesMerge , 
#       aes(x=Date , y = Weekly_Sales , color = Dept ) ) +
#  geom_point() +
#  scale_y_continuous(name="Weekly Sales" )
```

\includegraphics[scale=.3]{allPoint-1}
<IMAGE src="Images/allPoint-1.png" />

While it is not easy to make sense of a graph with more than 400 thousand data points, here are some salient features that stand out:

* Departments with the higher numbers (<code>Dept>=75</code>) have higher sales figures than the lower numbered departments (<code>Dept <=25</code>)
* During Christmas we see a spike in a lower numbered department's sale
* We see a repeating annual pattern. Possibly indicating that Week Numbers (eg: 50th week of the year) may be an important predictor variable

```{r makeWeekNumberMonthNumber}
## Adding the Week Number to the holidayDateTableDataFrame
holidayDateTableDataFrame$WeekNumber <- weekNumber( holidayDateTableDataFrame$Date )
## Adding Month to holidayDateTableDataFrame
holidayDateTableDataFrame$Month <- month( holidayDateTableDataFrame$Date )
```

We have created Week and month numbers to influence the regression model we will develop in Section 5.

```{r mergingholidayDateTableDataFrameWithTrainStoresFeaturesMerge}
## Merging holidayDateTableDataFrame with trainStoresFeaturesMerge
trainStoresFeaturesMerge <- 
  merge( trainStoresFeaturesMerge , holidayDateTableDataFrame , by = "Date" )
## removing holidayDateTableDataFrame from memory
rm( holidayDateTableDataFrame )
```

\pagebreak
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break

# 4. Stage 2: Formal Statistical Inferences
## 4.1 On Average, Do Holiday Weeks Spike Sales Up?
We would like to investigate if the sales figures are statistically higher during holiday weeks. We have seen earlier that visually it does appear that sales spike up during Thanksgiving and Christmas. We would like to statistically verify this.

### The Hypotheses
Let us state our Hypotheses:

* <B>Null Hypothesis (H~0~)</B> : On average, there is no difference in weekly sales figures during holiday weeks. In other words, there is no statisically significant difference between  <code>Weekly_Sales</code> numbers between holiday weeks and non-holiday weeks
* <B>Alternate Hypothesis (H~A~)</B>: Our alternate hypothesis is that there is a statistically significant higher sales figure during holiday weeks (one-sided test)

Mathematically, the hypothese are expressed below:

* H~0~: μ~diff~ = 0
* H~A~: μ~diff~ > 0

### 4.1.2 The Data
We need to separate the datasets into Holiday and Non-holiday weeks and then calculate the point estimate.

```{r hypDataSubset}
## Creating the datasets for Holiday and NotHoliday
Hyp_NotHoliday <- subset( trainStoresFeaturesMerge , IsHoliday == FALSE );
Hyp_Holiday <- subset( trainStoresFeaturesMerge , IsHoliday == TRUE );

## Getting the Number of rows in each dataset
nrow( Hyp_NotHoliday )
nrow( Hyp_Holiday )
```

### 4.1.3 Central Limit Theorm: Checking the Conditions for Hypothesis Testing for Paired Data
The conditions for hypothesis testing:

* <B>Independence:</B> Sampled observations must be independent. Random sample must be collected and if it is without replacement then the sample size must be less than 10% of the Population
* <B>Sample Size / Skew: </B> The no of elements must be more than 30.

We select a size of <code>2500</code> which is less than 10% of <code>Hyp_Holiday</code>.

```{r samplingHyp}
## Number of sample elements to collect from population 
## should be <10% of holiday Week Population
ndiff <- 2500
## Seeding to ensure the randomness can be repeated
set.seed(1101)
## Getting a sample of elements (ndiff) (<10% of Holiday Weeks)
Holiday_Sample <- sample( Hyp_Holiday$Log_Weekly_Sales , ndiff )
head(Holiday_Sample)
NotHoliday_Sample <- sample( Hyp_NotHoliday$Log_Weekly_Sales , ndiff )
head(NotHoliday_Sample)
```

```{r visualizingTheSamplesCollected}
## combining both the sample into one x-Axis Variable
xVar <- c(NotHoliday_Sample , Holiday_Sample )
## Creating the color Variable
colorVar <- as.factor(c(rep(1, ndiff), rep(2, ndiff ) ) )
## creating the dataframe
sampleDensityDf <- data.frame( xVar ,  colorVar )
## the density plot showing the 
## Not Holiday and Holiday values of Log(Weekly_Sales)
plottingDensity <- ggplot( sampleDensityDf , aes(x = xVar, fill = colorVar) ) + 
  geom_density( alpha = .2 ) +
  scale_x_continuous( "log(Weekly_Sales)" ) +
  scale_fill_discrete( 
    name = "Sample" , labels=c( "Not Holiday", "Holiday" ) ) +
  scale_y_continuous( "Density" ) +
  theme( legend.position = "bottom" )
## box plot to show the Density Distribution
boxPlotDensity <- ggplot( sampleDensityDf , aes( colorVar , xVar ) ) + 
  geom_boxplot( aes( fill = colorVar ) ) + 
  scale_y_continuous( "log(Weekly_Sales)" ) +
  scale_fill_discrete( 
    name = "Sample" , labels=c( "Not Holiday", "Holiday" ) ) +
  scale_x_discrete( "Sample" , labels=c( "Not Holiday", "Holiday" )  ) +
  theme( legend.position = "bottom" )
## arranging the plots next to each other
grid.arrange( plottingDensity , boxPlotDensity , nrow = 1 )
## removing plots from memory
rm( xVar , colorVar , sampleDensityDf , plottingDensity , 
    boxPlotDensity, Hyp_Holiday , Hyp_NotHoliday)
```

### 4.1.4 Calculating the Test Statistic
```{r calculTestStatistic}
## Calculating the Difference
Diff_Log_Weekly_Sales = Holiday_Sample - NotHoliday_Sample
## Printing Top 5 values of diff
head(Diff_Log_Weekly_Sales)
## Calculating the Test Statistic
xBar <- mean(Diff_Log_Weekly_Sales)
xBar
## Calculating the Test Statistic
zScore <- xBar / standardError(Diff_Log_Weekly_Sales)
zScore
## Calculating p-value
## 1-pnorm() because we are doing a one-sided test - greater than
pValue <- 1-pnorm( zScore ) 
pValue
## removing variables not needed anymore
rm( pValue , zScore , xBar , Diff_Log_Weekly_Sales , Holiday_Sample , 
    NotHoliday_Sample , ndiff )
```

### 4.1.5 Decision: Alternate Hypothesis (H~A~) is Rejected
The **Null Hypothesis (H~0~)** is NOT rejected because the <code>pValue</code> is greater than the significance value of <code>0.05</code>.

This imples that the Alternate Hypothesis (H~A~) is rejected. Holiday weeks do not cause sales to spike up.

### 4.1.6 Real World Application
However, in graphs drawn in the section *'Total Sales Per Week'* we were presented with another reality. We saw the spike in sales during holiday season - Thanksgiving and Christmas. One will recognize on close examination of the graphs that most of the sales spike happened 1 week before the holiday week. Sales during the holiday week was mostly on the decline from the high sales peak from the week before.

Therefore, statistically, Holiday Week Sales are not very different from that of non-holiday week sales. However, if we consider Holiday Season sales, it may tell a different story. This will form the basis of our next Hypothesis test - on average do Thanksgiving and Christmas contribute to spike in sales?

\pagebreak

## 4.2 On Average, Do Thanksgiving and Christmas Contribute to Sales Spikes?
As stated in the previous section, we would like to verify the behavior of sales spiking up during Holiday seasons, rather than just the holidays themselves. As we noted in the previous section, the holiday week itself may be no different from the rest of the dataset, but the holiday season could be interesting to study.

For the purposes of our study we will take 2 weeks before and after as a part of the holiday season. We will consider only Thanksgiving & Christmas - from the graph in section *'Total Sales Per Week'* the other holidays don't influence the sales as much.

### 4.2.1 The Hypotheses
Let us state our Hypotheses:

* <B>Null Hypothesis (H~0~)</B> : On average, there is no difference in weekly sales figures during holiday seasons. In other words, there is no statisically significant difference between  <code>Weekly_Sales</code> numbers between holiday season and non-holiday seasons.
* <B>Alternate Hypothesis (H~A~)</B>: Our alternate hypothesis is that there is a statistically significant higher sales figure during holiday season (one-sided test)

Mathematically, the hypothese are expressed below:

* H~0~: μ~diff~ = 0
* H~A~: μ~diff~ > 0

### 4.2.2 The Data
We need to separate the datasets into Holiday and Non-holiday seasons and then calculate the point estimate.

```{r hypDataSubset2}
## Creating the datasets for Holiday and NotHoliday
Hyp_NotHoliday <- subset( trainStoresFeaturesMerge , 
                          HolidaySeasonId != 11 & HolidaySeasonId != 12  );
Hyp_Holiday <- subset( trainStoresFeaturesMerge , 
                       HolidaySeasonId == 11 | HolidaySeasonId == 12);

## Getting the Number of rows in each dataset
nrow( Hyp_NotHoliday )
nrow( Hyp_Holiday )
```

### 4.2.3 Central Limit Theorm: Checking the Conditions for Hypothesis Testing for Paired Data
The conditions for hypothesis testing:

* <B>Independence:</B> Sampled observations must be independent. Random sample must be collected and if it is without replacement then the sample size must be less than 10% of the Population
* <B>Sample Size / Skew: </B> The no of elements must be more than 30.

We select a size of <code>5000</code> which is less than 10% of <code>Hyp_Holiday</code>.

```{r samplingHyp2}
## Number of sample elements to collect from population 
## should be <10% of holiday Week Population
ndiff <- 5000
## Seeding to ensure the randomness can be repeated
set.seed(1101)
## Getting a sample of elements (ndiff) (<10% of Holiday Weeks)
Holiday_Sample <- sample( Hyp_Holiday$Log_Weekly_Sales , ndiff )
head(Holiday_Sample)
NotHoliday_Sample <- sample( Hyp_NotHoliday$Log_Weekly_Sales , ndiff )
head(NotHoliday_Sample)
```

```{r visualizingTheSamplesCollected2}
## combining both the sample into one x-Axis Variable
xVar <- c(NotHoliday_Sample , Holiday_Sample )
## Creating the color Variable
colorVar <- as.factor(c(rep(1, ndiff), rep(2, ndiff ) ) )
## creating the dataframe
sampleDensityDf <- data.frame( xVar ,  colorVar )
## the density plot showing the 
## Not Holiday and Holiday values of Log(Weekly_Sales)
plottingDensity <- ggplot( sampleDensityDf , aes(x = xVar, fill = colorVar) ) + 
  geom_density( alpha = .2 ) +
  scale_x_continuous( "log(Weekly_Sales)" ) +
  scale_fill_discrete( 
    name = "Sample" , labels=c( "Not Holiday", "Holiday Season" ) ) +
  scale_y_continuous( "Density" ) +
  theme( legend.position = "bottom" )
## box plot to show the Density Distribution
boxPlotDensity <- ggplot( sampleDensityDf , aes( colorVar , xVar ) ) + 
  geom_boxplot( aes( fill = colorVar ) ) + 
  scale_y_continuous( "log(Weekly_Sales)" ) +
  scale_fill_discrete( 
    name = "Sample" , labels=c( "Not Holiday", "Holiday" ) ) +
  scale_x_discrete( "Sample" , labels=c( "Not Holiday", "Holiday" ) ) +
  theme( legend.position = "bottom" )
## arranging the plots next to each other
grid.arrange( plottingDensity , boxPlotDensity , nrow = 1 )
## removing plots from memory
rm( xVar , colorVar , sampleDensityDf , plottingDensity , 
    boxPlotDensity, Hyp_Holiday , Hyp_NotHoliday)
```

### 4.2.4 Calculating the Test Statistic
```{r calculTestStatistic2}
## Calculating the Difference
Diff_Log_Weekly_Sales = Holiday_Sample - NotHoliday_Sample
## Printing Top 5 values of diff
head(Diff_Log_Weekly_Sales)
## Calculating the Test Statistic
xBar <- mean(Diff_Log_Weekly_Sales)
xBar
## Calculating the Test Statistic
zScore <- xBar / standardError(Diff_Log_Weekly_Sales)
zScore
## Calculating p-value
## 1-pnorm() because we are doing a one-sided test - greater than
pValue <- 1-pnorm( zScore ) 
pValue

## removing variables not needed anymore
rm( pValue , zScore , xBar , Diff_Log_Weekly_Sales , Holiday_Sample , NotHoliday_Sample , ndiff )
```

### 4.2.5 Decision: Null Hypothesis (H~0~) is Rejected
The **Null Hypothesis (H~0~)** is rejected because the <code>pValue</code> is much smaller than the significance value of <code>0.05</code>.

This imples that the Alternate Hypothesis (H~A~) is NOT rejected. Holiday seasons do cause a spike in sales.

### 4.2.6 Real World Application
This confirms the what we visually depicted in section *'Total Sales Per Week'* regarding sales spiking up during Christmas and Thanksgiving.

\pagebreak

## 4.3 Do Bigger Stores contribute to Higher Sales Figures?

\pagebreak
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
# 5. Stage 3: Linear Regression: Predicting Weekly_Sales

\pagebreak
# Diagnostic ------- REMOVE LATER
```{r diagnostic}
nrow(train)
nrow(trainStoresFeaturesMerge)

ls()
# removing all the data from memory
rm( list = ls() )
```

